{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3fe20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port', '0'). \\\n",
    "config('spark.shuffle.useOldFetchProtocol', 'true'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e20d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = \"order_id long , order_date string, customer_id long,order_status string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b742b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(orders_schema) \\\n",
    ".load(\"/public/trendytech/orders/orders_1gb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c002c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b334685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.groupBy(\"order_status\").count().write.format(\"csv\").mode(\"overwrite\").save(\"output101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f018f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.groupBy(\"order_status\").count().write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3741d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_schema = \"customerid long , customer_fname string , customer_lname string , user_name string,password string , address string, city string, state string, pincode long \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b5150a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(customer_schema) \\\n",
    ".load(\"/public/trendytech/retail_db/customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62db7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+---------+---------+--------------------+-------------+-----+-------+\n",
      "|customerid|customer_fname|customer_lname|user_name| password|             address|         city|state|pincode|\n",
      "+----------+--------------+--------------+---------+---------+--------------------+-------------+-----+-------+\n",
      "|         1|       Richard|     Hernandez|XXXXXXXXX|XXXXXXXXX|  6303 Heather Plaza|  Brownsville|   TX|  78521|\n",
      "|         2|          Mary|       Barrett|XXXXXXXXX|XXXXXXXXX|9526 Noble Embers...|    Littleton|   CO|  80126|\n",
      "|         3|           Ann|         Smith|XXXXXXXXX|XXXXXXXXX|3422 Blue Pioneer...|       Caguas|   PR|    725|\n",
      "|         4|          Mary|         Jones|XXXXXXXXX|XXXXXXXXX|  8324 Little Common|   San Marcos|   CA|  92069|\n",
      "|         5|        Robert|        Hudson|XXXXXXXXX|XXXXXXXXX|10 Crystal River ...|       Caguas|   PR|    725|\n",
      "|         6|          Mary|         Smith|XXXXXXXXX|XXXXXXXXX|3151 Sleepy Quail...|      Passaic|   NJ|   7055|\n",
      "|         7|       Melissa|        Wilcox|XXXXXXXXX|XXXXXXXXX|9453 High Concession|       Caguas|   PR|    725|\n",
      "|         8|         Megan|         Smith|XXXXXXXXX|XXXXXXXXX|3047 Foggy Forest...|     Lawrence|   MA|   1841|\n",
      "|         9|          Mary|         Perez|XXXXXXXXX|XXXXXXXXX| 3616 Quaking Street|       Caguas|   PR|    725|\n",
      "|        10|       Melissa|         Smith|XXXXXXXXX|XXXXXXXXX|8598 Harvest Beac...|     Stafford|   VA|  22554|\n",
      "|        11|          Mary|       Huffman|XXXXXXXXX|XXXXXXXXX|    3169 Stony Woods|       Caguas|   PR|    725|\n",
      "|        12|   Christopher|         Smith|XXXXXXXXX|XXXXXXXXX|5594 Jagged Ember...|  San Antonio|   TX|  78227|\n",
      "|        13|          Mary|       Baldwin|XXXXXXXXX|XXXXXXXXX|7922 Iron Oak Gar...|       Caguas|   PR|    725|\n",
      "|        14|     Katherine|         Smith|XXXXXXXXX|XXXXXXXXX|5666 Hazy Pony Sq...|  Pico Rivera|   CA|  90660|\n",
      "|        15|          Jane|          Luna|XXXXXXXXX|XXXXXXXXX|    673 Burning Glen|      Fontana|   CA|  92336|\n",
      "|        16|       Tiffany|         Smith|XXXXXXXXX|XXXXXXXXX|      6651 Iron Port|       Caguas|   PR|    725|\n",
      "|        17|          Mary|      Robinson|XXXXXXXXX|XXXXXXXXX|     1325 Noble Pike|       Taylor|   MI|  48180|\n",
      "|        18|        Robert|         Smith|XXXXXXXXX|XXXXXXXXX|2734 Hazy Butterf...|     Martinez|   CA|  94553|\n",
      "|        19|     Stephanie|      Mitchell|XXXXXXXXX|XXXXXXXXX|3543 Red Treasure...|       Caguas|   PR|    725|\n",
      "|        20|          Mary|         Ellis|XXXXXXXXX|XXXXXXXXX|      4703 Old Route|West New York|   NJ|   7093|\n",
      "+----------+--------------+--------------+---------+---------+--------------------+-------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c35ff840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e527a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10485760b'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.autoBroadcastJoinThreshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4298dc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10485760 / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07b3c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.autoBroadcastJoinThreshold', '-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.autoBroadcastJoinThreshold', '10485760b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db966de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+------------+----------+--------------+--------------+---------+---------+--------------------+--------+-----+-------+\n",
      "|order_id|          order_date|customer_id|order_status|customerid|customer_fname|customer_lname|user_name| password|             address|    city|state|pincode|\n",
      "+--------+--------------------+-----------+------------+----------+--------------+--------------+---------+---------+--------------------+--------+-----+-------+\n",
      "|   64185|2014-03-28 00:00:...|         26|     PENDING|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   21730|2013-12-05 00:00:...|         26|      CLOSED|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   26441|2014-01-05 00:00:...|         26|    COMPLETE|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   30480|2014-01-30 00:00:...|         26|     PENDING|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   34653|2014-02-23 00:00:...|         26|    COMPLETE|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   64185|2014-03-28 00:00:...|         26|     PENDING|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   21730|2013-12-05 00:00:...|         26|      CLOSED|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   26441|2014-01-05 00:00:...|         26|    COMPLETE|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   30480|2014-01-30 00:00:...|         26|     PENDING|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   34653|2014-02-23 00:00:...|         26|    COMPLETE|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   64185|2014-03-28 00:00:...|         26|     PENDING|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   21730|2013-12-05 00:00:...|         26|      CLOSED|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   26441|2014-01-05 00:00:...|         26|    COMPLETE|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   30480|2014-01-30 00:00:...|         26|     PENDING|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   34653|2014-02-23 00:00:...|         26|    COMPLETE|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   64185|2014-03-28 00:00:...|         26|     PENDING|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   21730|2013-12-05 00:00:...|         26|      CLOSED|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   26441|2014-01-05 00:00:...|         26|    COMPLETE|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   30480|2014-01-30 00:00:...|         26|     PENDING|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "|   34653|2014-02-23 00:00:...|         26|    COMPLETE|        26|        Johnny|          Hood|XXXXXXXXX|XXXXXXXXX|9576 Middle Hills...|Glenview|   IL|  60025|\n",
      "+--------+--------------------+-----------+------------+----------+--------------+--------------+---------+---------+--------------------+--------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.join(customers_df, orders_df.customer_id == customers_df.customerid, \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e994cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.join(customers_df, orders_df.customer_id == customers_df.customerid, \"inner\").write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ef57632",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.join(customers_df, orders_df.customer_id == customers_df.customerid, \"right\").write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02105296",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.join(customers_df, orders_df.customer_id == customers_df.customerid, \"left\").write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad19d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.join(orders_df, orders_df.customer_id == customers_df.customerid, \"left\").write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "245edb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d469b94b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o147.save.\n: java.util.concurrent.ExecutionException: org.apache.spark.util.SparkFatalException\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:194)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:515)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:203)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareRelation(BroadcastHashJoinExec.scala:217)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter(HashJoin.scala:503)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter$(HashJoin.scala:502)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume(HashJoin.scala:358)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume$(HashJoin.scala:355)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:352)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:351)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:338)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.writeWithV2(WriteToDataSourceV2Exec.scala:241)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.run(WriteToDataSourceV2Exec.scala:255)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:377)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.util.SparkFatalException\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:168)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:185)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-014e60284e59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcustomers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morders_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morders_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustomer_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcustomers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustomerid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"noop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o147.save.\n: java.util.concurrent.ExecutionException: org.apache.spark.util.SparkFatalException\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:194)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:515)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:203)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareRelation(BroadcastHashJoinExec.scala:217)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter(HashJoin.scala:503)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter$(HashJoin.scala:502)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume(HashJoin.scala:358)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume$(HashJoin.scala:355)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:352)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:351)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:338)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.writeWithV2(WriteToDataSourceV2Exec.scala:241)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.run(WriteToDataSourceV2Exec.scala:255)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:377)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.util.SparkFatalException\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:168)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:185)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "customers_df.join(broadcast(orders_df), orders_df.customer_id == customers_df.customerid, \"left\").write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea73e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
